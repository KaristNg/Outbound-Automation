{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291a25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List\n",
    "from urllib.parse import urlparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11aeee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_websites_from_csv(csv_file_path: str) -> List[Dict[str, str]]:\n",
    "# Function to scrape CSV and domains in it -> to return parsed HTML content.\n",
    "    scraped_data = []\n",
    "\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader, None)  # Skip the header row if it exists\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if not row:  # Skip empty rows\n",
    "                continue\n",
    "            \n",
    "            url = row[0].strip()  # Assuming the URL is in the first column\n",
    "            if not url.startswith('http'):\n",
    "                url = 'http://' + url\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                data = {\n",
    "                    'url': url,\n",
    "                    'domain': urlparse(url).netloc,\n",
    "                    'title': soup.title.string if soup.title else 'No title found',\n",
    "                    'meta_description': soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else 'No meta description found',\n",
    "                    'headings': [h.text for h in soup.find_all(['h1', 'h2', 'h3'])[:5]],\n",
    "                    'paragraphs': [p.text for p in soup.find_all('p')[:3]]\n",
    "                }\n",
    "                \n",
    "                scraped_data.append(data)\n",
    "                print(f\"Successfully scraped: {url}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {url}: {str(e)}\")\n",
    "                scraped_data.append({\n",
    "                    'url': url,\n",
    "                    'domain': urlparse(url).netloc,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "def analyze_content(data: Dict[str, str]) -> List[str]:\n",
    "    #Analyze scraped content to identify key themes and topics.\n",
    "    relevant_content = []\n",
    "    \n",
    "    keywords = ['achievement','professional','proud','remote work', 'solution']\n",
    "    \n",
    "    all_text = ' '.join(data['headings'] + data['paragraphs']).lower()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in all_text:\n",
    "            relevant_content.append(f\"Website mentions {keyword}\")\n",
    "    \n",
    "    return relevant_content\n",
    "\n",
    "def generate_personalized_email(data: Dict[str, str], relevant_content: List[str]) -> str:\n",
    "#Generic Personalized message using the relevant HTML content (to be replaced by OpenAI prompt)\n",
    "  return f\"Subject: Innovative Workspace Solutions for {data['domain']}\\n\\n\" \\\n",
    "           f\"Dear {data['domain']} Team,\\n\\n\" \\\n",
    "           f\"I hope this email finds you well. I recently came across your website and was impressed by {data['title']}.\\n\\n\" \\\n",
    "           f\"At Litespace, we specialize in providing innovative workspace solutions that enhance productivity and collaboration, \" \\\n",
    "           f\"especially in remote work environments. Given your focus on {', '.join(relevant_content)}, I believe our services \" \\\n",
    "           f\"could be particularly beneficial for your team.\\n\\n\" \\\n",
    "           f\"Would you be interested in scheduling a brief call to discuss how Litespace can support your specific needs?\\n\\n\" \\\n",
    "           f\"Best regards,\\n\" \\\n",
    "           f\"The Litespace Team\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a165809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to your CSV file containing website URLs: website.csv\n",
      "Successfully scraped: https://www.litespace.io/\n",
      "Successfully scraped: https://www.hoppier.com/\n",
      "[{'url': 'https://www.litespace.io/', 'domain': 'www.litespace.io', 'title': 'Home - Litespace', 'meta_description': 'Litespace: Your all-in-one platform for fostering team connection, alignment, and building engaging, inclusive work environments.', 'headings': ['Personalize Your Employee Experience to \\u200dDrive Performance', 'The Ultimate Employee Experience', 'Engage Your Team', 'Automate Your Scheduling', 'Measure Your Impact'], 'paragraphs': ['Engagement', 'Equip your team with a range of tools to engage your team', 'Analytics']}, {'url': 'https://www.hoppier.com/', 'domain': 'www.hoppier.com', 'title': 'Hoppier | Easily Send Rewards and Gifts to 60+ Countries', 'meta_description': 'Hoppier makes employee rewards and client gifts easy! In a few clicks â€“ send lunch, coffee, and more. Trusted by HR Leaders and Revenue Teams in 60+ countries.', 'headings': ['From Tokyo to Toronto: Instantly Send Virtual Gift Cards to 60+ Countries and Get Unused Funds Back!', 'Rewards & Incentives for Global Remote Teams that Care.', 'Engage your virtual attendees, global team, and clients', 'Hoppier virtual cards let everyone order their favorite meals or gifts, anywhere in the world. Delivered by email in minutes.', 'The old way vs Hoppier way'], 'paragraphs': ['Create your remote lunch program, send a coffee, happy hour drinks, personal gifts, charity donations, learning allowances, and more. Hoppier works in over 60+ countries!', 'Hoppier works in 60+ countries for your remote employees and customers.', 'No control or tracking']}]\n",
      "\n",
      "Generated Personalized Email for www.litespace.io:\n",
      "-----------------------------\n",
      "Subject: Innovative Workspace Solutions for www.litespace.io\n",
      "\n",
      "Dear www.litespace.io Team,\n",
      "\n",
      "I hope this email finds you well. I recently came across your website and was impressed by Home - Litespace.\n",
      "\n",
      "At Litespace, we specialize in providing innovative workspace solutions that enhance productivity and collaboration, especially in remote work environments. Given your focus on , I believe our services could be particularly beneficial for your team.\n",
      "\n",
      "Would you be interested in scheduling a brief call to discuss how Litespace can support your specific needs?\n",
      "\n",
      "Best regards,\n",
      "The Litespace Team\n",
      "\n",
      "\n",
      "\n",
      "Generated Personalized Email for www.hoppier.com:\n",
      "-----------------------------\n",
      "Subject: Innovative Workspace Solutions for www.hoppier.com\n",
      "\n",
      "Dear www.hoppier.com Team,\n",
      "\n",
      "I hope this email finds you well. I recently came across your website and was impressed by Hoppier | Easily Send Rewards and Gifts to 60+ Countries.\n",
      "\n",
      "At Litespace, we specialize in providing innovative workspace solutions that enhance productivity and collaboration, especially in remote work environments. Given your focus on , I believe our services could be particularly beneficial for your team.\n",
      "\n",
      "Would you be interested in scheduling a brief call to discuss how Litespace can support your specific needs?\n",
      "\n",
      "Best regards,\n",
      "The Litespace Team\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    csv_file_path = input(\"Enter the path to your CSV file containing website URLs: \")\n",
    "    scraped_websites = scrape_websites_from_csv(csv_file_path)\n",
    "    print(scraped_websites)\n",
    "\t#Scrape domains in CSV file\n",
    "\n",
    "    # Process each scraped website\n",
    "    for website_data in scraped_websites:\n",
    "        if 'error' in website_data:\n",
    "            print(f\"Skipping {website_data['url']} due to error: {website_data['error']}\")\n",
    "            continue\n",
    "\n",
    "        relevant_content = analyze_content(website_data)\n",
    "        personalized_email = generate_personalized_email(website_data, relevant_content)\n",
    "\n",
    "        print(f\"\\nGenerated Personalized Email for {website_data['domain']}:\")\n",
    "        print(\"-----------------------------\")\n",
    "        print(personalized_email)\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b58886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13f4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
